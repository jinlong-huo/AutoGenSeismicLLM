{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "# Supervisor (LLM-based)\n",
    "supervisor = AssistantAgent(\n",
    "    name=\"supervisor\",\n",
    "    system_message=\"Analyze seismic data by coordinating fault detection and horizon picking. Validate results.\",\n",
    "    llm_config={\"config_list\": [...]},  # LLM 模型配置 (e.g., OpenAI, LiteLLM)\n",
    ")\n",
    "\n",
    "# User proxy for human input\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    code_execution_config={\"work_dir\": \"seismic\"},\n",
    ")\n",
    "\n",
    "# Register tools for models\n",
    "user_proxy.register_function(\n",
    "    function_map={\n",
    "        \"fault_detection\": fault_detection, # 断层检测代理\n",
    "        \"horizon_picking\": horizon_picking, # 地层拾取代理\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    supervisor,\n",
    "    message=\"Analyze the seismic data at path 'data/example.segy'. First detect faults, then pick horizons.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fault_detection(seismic_data: str) -> dict:\n",
    "    # Load your model and run inference\n",
    "    return {\"fault_locations\": [...]}\n",
    "\n",
    "def horizon_picking(seismic_data: str) -> dict:\n",
    "    # Run horizon picking model\n",
    "    return {\"horizons\": [...]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Configuration for local LLM endpoints\n",
    "def get_llm_config(model_path: str, model_type: str = \"llama\") -> Dict[str, Any]:\n",
    "    config = {\n",
    "        \"cache_seed\": 42,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 2000,\n",
    "        \"model\": model_type,\n",
    "        \"local_path\": model_path,\n",
    "    }\n",
    "    return config\n",
    "\n",
    "# Custom LLM wrapper for local models\n",
    "class LocalLLMWrapper:\n",
    "    def __init__(self, model_path: str, model_type: str = \"llama\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int = 2000) -> str:\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "# Define agent configurations\n",
    "def create_agents(local_llm_configs: List[Dict[str, Any]]):\n",
    "    # Supervisor agent using local LLama/Qwen/Phi\n",
    "    supervisor_config = {\n",
    "        \"llm_config\": local_llm_configs[0],  # Using first model as supervisor\n",
    "        \"system_message\": \"\"\"You are a supervisor for seismic interpretation tasks.\n",
    "        Your role is to coordinate between different specialist agents and ensure\n",
    "        the interpretation process follows best practices.\"\"\"\n",
    "    }\n",
    "    \n",
    "    # Seismic data preprocessor agent\n",
    "    preprocessor_config = {\n",
    "        \"llm_config\": local_llm_configs[1],  # Using second model\n",
    "        \"system_message\": \"\"\"You are a specialist in seismic data preprocessing.\n",
    "        Your tasks include data loading, noise reduction, and signal enhancement.\"\"\"\n",
    "    }\n",
    "    \n",
    "    # Seismic feature extraction agent\n",
    "    feature_extractor_config = {\n",
    "        \"llm_config\": local_llm_configs[2],  # Using third model\n",
    "        \"system_message\": \"\"\"You are a specialist in seismic feature extraction.\n",
    "        Your role is to identify and extract key seismic attributes and patterns.\"\"\"\n",
    "    }\n",
    "\n",
    "    # Create agents\n",
    "    supervisor = autogen.AssistantAgent(\n",
    "        name=\"SupervisorAgent\",\n",
    "        **supervisor_config\n",
    "    )\n",
    "    \n",
    "    preprocessor = autogen.AssistantAgent(\n",
    "        name=\"PreprocessorAgent\",\n",
    "        **preprocessor_config\n",
    "    )\n",
    "    \n",
    "    feature_extractor = autogen.AssistantAgent(\n",
    "        name=\"FeatureExtractorAgent\",\n",
    "        **feature_extractor_config\n",
    "    )\n",
    "    \n",
    "    # User proxy agent for interaction\n",
    "    user_proxy = autogen.UserProxyAgent(\n",
    "        name=\"UserProxy\",\n",
    "        human_input_mode=\"TERMINATE\",\n",
    "        max_consecutive_auto_reply=10\n",
    "    )\n",
    "    \n",
    "    return supervisor, preprocessor, feature_extractor, user_proxy\n",
    "\n",
    "# Main workflow\n",
    "def main():\n",
    "    # Setup local model configurations\n",
    "    model_configs = [\n",
    "        get_llm_config(\"/path/to/llama-model\", \"llama\"),\n",
    "        get_llm_config(\"/path/to/qwen-model\", \"qwen\"),\n",
    "        get_llm_config(\"/path/to/phi-model\", \"phi\")\n",
    "    ]\n",
    "    \n",
    "    # Initialize local LLM instances\n",
    "    llm_instances = [\n",
    "        LocalLLMWrapper(config[\"local_path\"], config[\"model\"])\n",
    "        for config in model_configs\n",
    "    ]\n",
    "    \n",
    "    # Create agents\n",
    "    supervisor, preprocessor, feature_extractor, user_proxy = create_agents(model_configs)\n",
    "    \n",
    "    # Define group chat\n",
    "    groupchat = autogen.GroupChat(\n",
    "        agents=[supervisor, preprocessor, feature_extractor, user_proxy],\n",
    "        messages=[],\n",
    "        max_round=50\n",
    "    )\n",
    "    \n",
    "    # Create group chat manager\n",
    "    manager = autogen.GroupChatManager(\n",
    "        groupchat=groupchat,\n",
    "        llm_config=model_configs[0]  # Using supervisor's config\n",
    "    )\n",
    "    \n",
    "    # Example: Initiate a seismic interpretation task\n",
    "    user_proxy.initiate_chat(\n",
    "        manager,\n",
    "        message=\"\"\"Please analyze the seismic data in file 'example.sgy':\n",
    "        1. Preprocess the data to remove noise\n",
    "        2. Extract key features and horizons\n",
    "        3. Provide interpretation results\"\"\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JLtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
